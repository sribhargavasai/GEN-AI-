{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAZvnGs_0YIn"
      },
      "source": [
        "### Connecting Google Colab with your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN71wB2jf7aG",
        "outputId": "a3ee2579-47ce-41dc-d766-336135baad4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdHcPZ3lth0F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/NLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox5RUFP2uW9h"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxJOt000uW9i"
      },
      "source": [
        "mk### Natural Language Toolkit\n",
        "\n",
        "* NLTK is a leading platform for building Python programs to work with human language data.\n",
        "* NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "* NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
        "* NLTK is available for Windows, Mac OS X, and Linux.\n",
        "* Best of all, NLTK is a free, open source, community-driven project. For more details - www.nltk.org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3ahz3jNlt76"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_qCz8TpRs4"
      },
      "source": [
        "```Python\n",
        "!pip install nltk\n",
        "\n",
        "nltk.download()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc98pnsluW9q"
      },
      "source": [
        "#### We will be using a sample text to demonstrate various text pre-processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZvpY-B2SuW9r"
      },
      "outputs": [],
      "source": [
        "string = '''At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
        "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
        "My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
        "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
        "The train was @09:30 AM and we have to reach the station by 08:30 AM. At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
        "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "PnbSWj1DuW9u",
        "outputId": "0812afca-20ba-43cd-8d14-713c98238067"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\\nMy companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\nThe train was @09:30 AM and we have to reach the station by 08:30 AM. At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG6MImkUuW9x",
        "outputId": "bc760803-7c5d-4c8b-dc5b-86ad755c2841"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVPjUaiguW90",
        "outputId": "e9a68d29-1b0b-483c-c20a-cc9bded1fa6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1118"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOqDmNDKuW94",
        "outputId": "f92ded28-0bd4-4abb-ecd0-d0bef1479f02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.count(\"Waterloo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aBo2YI4gfKL",
        "outputId": "7b83c8c0-00f2-45d7-8f0b-a0cdd9464f31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x=string.split()\n",
        "len(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW4XdyMVuW97"
      },
      "source": [
        "#### But what if we want to read the text from a file?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTMueASr1J__"
      },
      "source": [
        "Change the working directory using \".chdir()\" method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8euOIKtDuW9-"
      },
      "outputs": [],
      "source": [
        "# Change the working directory using \".chdir()\" method\n",
        "PATH = os.getcwd()\n",
        "DATA_PATH = os.path.join(PATH, \"data\")\n",
        "os.chdir(DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6I5xO2j1KAA"
      },
      "source": [
        "List the files that are present in the path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S98uu3XcoNJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHCRnlXtuW-B",
        "outputId": "ba8bce3e-7799-418b-c505-f265f9626c71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sample_text.txt']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln2FIprN1KAA"
      },
      "source": [
        "Reading from a text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FMref9PuW-G"
      },
      "outputs": [],
      "source": [
        "with open('sample_text.txt', 'r') as x:\n",
        "    string = x.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOShemtM1KAB"
      },
      "source": [
        "'r' stands for read operation.\n",
        "\n",
        "'w' to write to a file and\n",
        "\n",
        "'a' to append to an existing file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Ux9tkyBb1KAB",
        "outputId": "9e9fb0c4-8534-4db9-82bc-7d2671dcdff5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\nThe train was @09:30 AM and we had to reach the station by 08:30 AM. At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. '"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnlF5NJ1uW-M"
      },
      "source": [
        "#### Now that our sample text is ready, let us perform following steps:\n",
        "\n",
        "1. Sentence Tokenizing\n",
        "2. Word Tokenizing\n",
        "3. Stop Word Removal\n",
        "4. Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX51-aZSuW-N"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM09lTLD8kU5"
      },
      "source": [
        "Use the NLTK Downloader to obtain the punkt resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ5OQB4hjenD"
      },
      "outputs": [],
      "source": [
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTG-m1jurAIK",
        "outputId": "1ff2076f-5a06-403a-93e0-0aedc082c987"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G4C5jkY1KAC"
      },
      "source": [
        "#### sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMV9Z-Qc7yEF"
      },
      "source": [
        "sent_tokenize, return a sentence-tokenized copy of text, using NLTK's recommended sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80hwUN8o1KAC"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokens = sent_tokenize(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQnG1O6XuW-R",
        "outputId": "4587f24a-31b5-48fa-dce7-8bc13aa3a973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "print(type(sent_tokens))\n",
        "print(len(sent_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcBsJaHukHsL",
        "outputId": "6f5c750c-f30b-48ee-e34e-1d3b589ac2e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.',\n",
              " 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.',\n",
              " 'The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.',\n",
              " 'To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.',\n",
              " 'My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.',\n",
              " 'Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.',\n",
              " 'The train was @09:30 AM and we had to reach the station by 08:30 AM.',\n",
              " 'At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.',\n",
              " 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc25P3uiuW-P",
        "outputId": "3bf5302d-56c3-4e23-d746-4203dd6bf3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
            "\n",
            "\n",
            "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
            "\n",
            "\n",
            "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
            "\n",
            "\n",
            "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
            "\n",
            "\n",
            "My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
            "\n",
            "\n",
            "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
            "\n",
            "\n",
            "The train was @09:30 AM and we had to reach the station by 08:30 AM.\n",
            "\n",
            "\n",
            "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
            "\n",
            "\n",
            "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sent in sent_tokens:\n",
        "    print(sent)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtLEWovuW-T"
      },
      "source": [
        "\n",
        "#### word_tokenize\n",
        "    Return a tokenized copy of text, using NLTK's recommended word tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrp06vADuW-U",
        "outputId": "ccbf8aff-27c2-4147-fc5d-c251d7520303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'Mr.', 'Alfred', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.', 'The', 'train', 'was', '@', '09:30', 'AM', 'and', 'we', 'had', 'to', 'reach', 'the', 'station', 'by', '08:30', 'AM', '.', 'At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(string)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m69GaAIMuW-W",
        "outputId": "fed33b53-f18b-4843-88ad-c8076c06493a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead']\n"
          ]
        }
      ],
      "source": [
        "print(tokens[0:11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yClNDhdJuW-Y"
      },
      "source": [
        "## Regular-Expression Tokenizers\n",
        "\n",
        "#### What is Regular Expression?\n",
        "\n",
        "A RegEx or Regular Expression in a programming language is a special text string used for describing a search pattern. It is extremely useful for extracting information from text such as code, files, log, spreadsheets or even documents.\n",
        "\n",
        "While using the regular expression the first thing to recognize is that everything is essentially a character, and we are writing patterns to match a specific sequence of characters also referred as string. Ascii or latin letters are those that are on your keyboards and Unicode is used to match the foreign text. It includes digits and punctuation and all special characters like $#@!%, etc.\n",
        "\n",
        "A RegexpTokenizer splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:('\\w+|$[\\d.]+|\\S+') For more information or different variations -\n",
        "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
        "http://www.nltk.org/howto/tokenize.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3F8X0FFuW-Z"
      },
      "source": [
        "#### Regular Expressions\n",
        "\n",
        "Regular expressions can contain both special and ordinary characters.\n",
        "\n",
        "Most ordinary characters, like 'A', 'a', or '0', are the simplest regular expressions; they simply match themselves. You can concatenate ordinary characters, so last matches the string 'last'.\n",
        "\n",
        "\n",
        "* `\\d` - Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "* `\\D` - Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "* `\\s` - Matches any whitespace character;\n",
        "* `\\S` - Matches any non-whitespace character;\n",
        "* `\\w` - Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "* `\\W` - Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "The special characters are:\n",
        "\n",
        "* `'.'` - (Dot.) In the default mode, this matches any character except a newline. If the DOTALL flag has been specified, this matches any character including a newline.\n",
        "* `'^'` - (Caret.) Matches the start of the string, and in MULTILINE mode also matches immediately after each newline.\n",
        "* `'$'` - Matches the end of the string or just before the newline at the end of the string, and in MULTILINE mode also matches            before a newline.\n",
        "* `'*'` - Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. `ab*` will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s.\n",
        "* `'+'` - Causes the resulting RE to match 1 or more repetitions of the preceding RE. `ab+` will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n",
        "* `'?'` - Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either ‘a’ or ‘ab’.\n",
        "* `'\\'` - Either escapes special characters (permitting you to match characters like '*', '?', and so forth), or signals a special sequence; special sequences are discussed below.\n",
        "* `[]` - Used to indicate a set of characters. In a set: Example: Characters can be listed individually, e.g. [amk] will match 'a', 'm', or 'k'.\n",
        "* `'|'` - A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B. An arbitrary number of REs can be separated by the '|' in this way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSwTUjY0oWZ6"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL3AcYHaobSb"
      },
      "outputs": [],
      "source": [
        "x='this is a 90 thing in 9'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK7qg5RSuW-b"
      },
      "source": [
        "#### Examples using NLTK Regular Expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0BIDL8QuW-Z",
        "outputId": "4f7046ee-6a19-4faa-867a-04eb91ae5ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', 'It', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'My', 'companion', 'Mr', 'Alfred', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'Suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows', 'The', 'train', 'was', '09', '30', 'AM', 'and', 'we', 'had', 'to', 'reach', 'the', 'station', 'by', '08', '30', 'AM', 'At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', 'It', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import RegexpTokenizer as regextoken\n",
        "\n",
        "tokenizer = regextoken(\"\\w+\")\n",
        "\n",
        "tokens = tokenizer.tokenize(string)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqN8VnYluEb2",
        "outputId": "d8939763-d3ca-442e-842b-2b65e50972a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Atwaterloo', 'we', 'werefortunate']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x='Atwaterloo we werefortunate'\n",
        "t=regextoken('\\w+')\n",
        "t.tokenize(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE4lM6NRtqSn",
        "outputId": "252f889b-cb2a-42cd-e6ef-e1283ba978ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 't', 'W', 'a', 't', 'e', 'r', 'l', 'o', 'o', 'w', 'e', 'w', 'e', 'r', 'e', 'f', 'o', 'r', 't', 'u', 'n', 'a', 't', 'e', 'i', 'n', 'c', 'a', 't', 'c', 'h', 'i', 'n', 'g', 'a', 't', 'r', 'a', 'i', 'n', 'f', 'o', 'r', 'L', 'e', 'a', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'd', 'w', 'h', 'e', 'r', 'e', 'w', 'e', 'h', 'i', 'r', 'e', 'd', 'a', 't', 'r', 'a', 'p', 'a', 't', 't', 'h', 'e', 's', 't', 'a', 't', 'i', 'o', 'n', 'i', 'n', 'n', 'a', 'n', 'd', 'd', 'r', 'o', 'v', 'e', 'f', 'o', 'r', 'f', 'o', 'u', 'r', 'o', 'r', 'f', 'i', 'v', 'e', 'm', 'i', 'l', 'e', 's', 't', 'h', 'r', 'o', 'u', 'g', 'h', 't', 'h', 'e', 'l', 'o', 'v', 'e', 'l', 'y', 'S', 'u', 'r', 'r', 'e', 'y', 'l', 'a', 'n', 'e', 's', 'I', 't', 'w', 'a', 's', 'a', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'd', 'a', 'y', 'w', 'i', 't', 'h', 'a', 'b', 'r', 'i', 'g', 'h', 't', 's', 'u', 'n', 'a', 'n', 'd', 'a', 'f', 'e', 'w', 'f', 'l', 'e', 'e', 'c', 'y', 'c', 'l', 'o', 'u', 'd', 's', 'i', 'n', 't', 'h', 'e', 'h', 'e', 'a', 'v', 'e', 'n', 's', 'T', 'h', 'e', 't', 'r', 'e', 'e', 's', 'a', 'n', 'd', 'w', 'a', 'y', 's', 'i', 'd', 'e', 'h', 'e', 'd', 'g', 'e', 's', 'w', 'e', 'r', 'e', 'j', 'u', 's', 't', 't', 'h', 'r', 'o', 'w', 'i', 'n', 'g', 'o', 'u', 't', 't', 'h', 'e', 'i', 'r', 'f', 'i', 'r', 's', 't', 'g', 'r', 'e', 'e', 'n', 's', 'h', 'o', 'o', 't', 's', 'a', 'n', 'd', 't', 'h', 'e', 'a', 'i', 'r', 'w', 'a', 's', 'f', 'u', 'l', 'l', 'o', 'f', 't', 'h', 'e', 'p', 'l', 'e', 'a', 's', 'a', 'n', 't', 's', 'm', 'e', 'l', 'l', 'o', 'f', 't', 'h', 'e', 'm', 'o', 'i', 's', 't', 'e', 'a', 'r', 't', 'h', 'T', 'o', 'm', 'e', 'a', 't', 'l', 'e', 'a', 's', 't', 't', 'h', 'e', 'r', 'e', 'w', 'a', 's', 'a', 's', 't', 'r', 'a', 'n', 'g', 'e', 'c', 'o', 'n', 't', 'r', 'a', 's', 't', 'b', 'e', 't', 'w', 'e', 'e', 'n', 't', 'h', 'e', 's', 'w', 'e', 'e', 't', 'p', 'r', 'o', 'm', 'i', 's', 'e', 'o', 'f', 't', 'h', 'e', 's', 'p', 'r', 'i', 'n', 'g', 'a', 'n', 'd', 't', 'h', 'i', 's', 's', 'i', 'n', 'i', 's', 't', 'e', 'r', 'q', 'u', 'e', 's', 't', 'u', 'p', 'o', 'n', 'w', 'h', 'i', 'c', 'h', 'w', 'e', 'w', 'e', 'r', 'e', 'e', 'n', 'g', 'a', 'g', 'e', 'd', 'M', 'y', 'c', 'o', 'm', 'p', 'a', 'n', 'i', 'o', 'n', 'M', 'r', 'A', 'l', 'f', 'r', 'e', 'd', 's', 'a', 't', 'i', 'n', 't', 'h', 'e', 'f', 'r', 'o', 'n', 't', 'o', 'f', 't', 'h', 'e', 't', 'r', 'a', 'p', 'h', 'i', 's', 'a', 'r', 'm', 's', 'f', 'o', 'l', 'd', 'e', 'd', 'h', 'i', 's', 'h', 'a', 't', 'p', 'u', 'l', 'l', 'e', 'd', 'd', 'o', 'w', 'n', 'o', 'v', 'e', 'r', 'h', 'i', 's', 'e', 'y', 'e', 's', 'a', 'n', 'd', 'h', 'i', 's', 'c', 'h', 'i', 'n', 's', 'u', 'n', 'k', 'u', 'p', 'o', 'n', 'h', 'i', 's', 'b', 'r', 'e', 'a', 's', 't', 'b', 'u', 'r', 'i', 'e', 'd', 'i', 'n', 't', 'h', 'e', 'd', 'e', 'e', 'p', 'e', 's', 't', 't', 'h', 'o', 'u', 'g', 'h', 't', 'S', 'u', 'd', 'd', 'e', 'n', 'l', 'y', 'h', 'o', 'w', 'e', 'v', 'e', 'r', 'h', 'e', 's', 't', 'a', 'r', 't', 'e', 'd', 't', 'a', 'p', 'p', 'e', 'd', 'm', 'e', 'o', 'n', 't', 'h', 'e', 's', 'h', 'o', 'u', 'l', 'd', 'e', 'r', 'a', 'n', 'd', 'p', 'o', 'i', 'n', 't', 'e', 'd', 'o', 'v', 'e', 'r', 't', 'h', 'e', 'm', 'e', 'a', 'd', 'o', 'w', 's', 'T', 'h', 'e', 't', 'r', 'a', 'i', 'n', 'w', 'a', 's', '0', '9', '3', '0', 'A', 'M', 'a', 'n', 'd', 'w', 'e', 'h', 'a', 'd', 't', 'o', 'r', 'e', 'a', 'c', 'h', 't', 'h', 'e', 's', 't', 'a', 't', 'i', 'o', 'n', 'b', 'y', '0', '8', '3', '0', 'A', 'M', 'A', 't', 'W', 'a', 't', 'e', 'r', 'l', 'o', 'o', 'w', 'e', 'w', 'e', 'r', 'e', 'f', 'o', 'r', 't', 'u', 'n', 'a', 't', 'e', 'i', 'n', 'c', 'a', 't', 'c', 'h', 'i', 'n', 'g', 'a', 't', 'r', 'a', 'i', 'n', 'f', 'o', 'r', 'L', 'e', 'a', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'd', 'w', 'h', 'e', 'r', 'e', 'w', 'e', 'h', 'i', 'r', 'e', 'd', 'a', 't', 'r', 'a', 'p', 'a', 't', 't', 'h', 'e', 's', 't', 'a', 't', 'i', 'o', 'n', 'i', 'n', 'n', 'a', 'n', 'd', 'd', 'r', 'o', 'v', 'e', 'f', 'o', 'r', 'f', 'o', 'u', 'r', 'o', 'r', 'f', 'i', 'v', 'e', 'm', 'i', 'l', 'e', 's', 't', 'h', 'r', 'o', 'u', 'g', 'h', 't', 'h', 'e', 'l', 'o', 'v', 'e', 'l', 'y', 'S', 'u', 'r', 'r', 'e', 'y', 'l', 'a', 'n', 'e', 's', 'I', 't', 'w', 'a', 's', 'a', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'd', 'a', 'y', 'w', 'i', 't', 'h', 'a', 'b', 'r', 'i', 'g', 'h', 't', 's', 'u', 'n', 'a', 'n', 'd', 'a', 'f', 'e', 'w', 'f', 'l', 'e', 'e', 'c', 'y', 'c', 'l', 'o', 'u', 'd', 's', 'i', 'n', 't', 'h', 'e', 'h', 'e', 'a', 'v', 'e', 'n', 's']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import RegexpTokenizer as regextoken\n",
        "t=regextoken('\\w')\n",
        "to=t.tokenize(string)\n",
        "print(to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7PSSd0YvATV"
      },
      "outputs": [],
      "source": [
        "st='This is A test example To understand cap tokenizer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5-a0RpduW-c",
        "outputId": "a574c6c2-234f-44c9-bb9c-93d09b4877e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'To']\n"
          ]
        }
      ],
      "source": [
        "capword_tokenizer = regextoken('[A-Z]\\w+')\n",
        "\n",
        "tokens_cap = capword_tokenizer.tokenize(st)\n",
        "print(tokens_cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uega5Zfsu81a",
        "outputId": "46290ba4-2ee1-4ed2-abd8-fb7c6fd611d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'A', 'To']\n"
          ]
        }
      ],
      "source": [
        "capword_tokenizer = regextoken('[A-Z]\\w*')\n",
        "\n",
        "tokens_cap = capword_tokenizer.tokenize(st)\n",
        "print(tokens_cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W68IpKANvlDZ",
        "outputId": "e7223785-b472-48cb-d5f3-378a3fcdc7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'Waterloo', 'Leatherhead', 'Surrey', 'It', 'The', 'To', 'My', 'Mr', 'Alfred', 'Suddenly', 'The', 'AM', 'AM', 'At', 'Waterloo', 'Leatherhead', 'Surrey', 'It']\n"
          ]
        }
      ],
      "source": [
        "tokens_cap = capword_tokenizer.tokenize(string)\n",
        "print(tokens_cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTUKwAzuW-e",
        "outputId": "33300891-5edd-48e6-c96e-e5ad3381ef88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fleecy', 'trees', 'green', 'between', 'sweet', 'deepest', 'fleecy']\n"
          ]
        }
      ],
      "source": [
        "text_tokenizer = regextoken('\\w*ee\\w*')\n",
        "\n",
        "tokens_text = text_tokenizer.tokenize(string)\n",
        "print(tokens_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15LUwIFLv_wy",
        "outputId": "6fbbb15a-f889-44e2-e1b9-f95862d23386"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['teen', 'tween', 'twee']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_tokenizer = regextoken('\\w+ee\\w*')\n",
        "st='teen and tween twee eehh'\n",
        "text_tokenizer.tokenize(st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBUWtXsRuW-g",
        "outputId": "434f613f-a95a-498e-de04-0776dd152ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['09', '30', '08', '30']\n"
          ]
        }
      ],
      "source": [
        "numeric_tokenizer = regextoken('[0-9]\\d*')\n",
        "\n",
        "tokens_numeric = numeric_tokenizer.tokenize(string)\n",
        "print(tokens_numeric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apRHIbBduW-i"
      },
      "source": [
        "#### Regular Expressions using \"re\"\n",
        "\n",
        "\"re\" module included with Python is primarily used for string searching and manipulation. It is quite useful for text extraction and pre-processing. The most common use for \"__re__\" is to search for patterns in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDrf7vNLuW-j"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = '''International School of Engineering (INSOFE) is an Applied Engineering school with area of focus in Data Science. It is located in Hyderabad, Bengaluru and Mumbai. It opened in 2011.\n",
        "The program is delivered through classroom only sessions and is suitable for students and working professionals. Dr. Dakshinamurthy V Kolluru, Dr. Sridhar Pappu and A S L Ganapathi Kumar started the institution in Hyderabad in mid-2011 and expanded to Bengaluru in early-2016. Initially the school functioned under mentorship of Dr. Dakshinamurthy, Dr. Sridhar and Dr. Sreerama Murthy. They are now supported by a team of additional mentors and in-house data scientists.\n",
        "In 2012, INSOFE also started Corporate training services. It extended operations to Bengaluru in 2016. CIO.com listed INSOFE 3rd in their list of \"16 Big Data Certifications That Will Pay Off\" consecutively from 2013-2016. Silicon India Magazine listed INSOFE in their list of \"Top 5 Big Data Training Institutes 2016\". Analytics India Magazine, listed INSOFE in \"Top 9 Analytics Training Institutes in India in 2016\". KDnuggets mentioned INSOFE in their list of Certificates in Analytics, Data Mining, and Data Science in 2014.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7AeevAyuW-k",
        "outputId": "ce53966c-6f9a-457d-ccf5-122bbf80536f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['International', 'School', 'of', 'Engineering', '(', 'INSOFE', ')', 'is', 'an', 'Applied', 'Engineering', 'school', 'with', 'area', 'of', 'focus', 'in', 'Data', 'Science', '.', 'It', 'is', 'located', 'in', 'Hyderabad', ',', 'Bengaluru', 'and', 'Mumbai', '.', 'It', 'opened', 'in', '2011', '.', 'The', 'program', 'is', 'delivered', 'through', 'classroom', 'only', 'sessions', 'and', 'is', 'suitable', 'for', 'students', 'and', 'working', 'professionals', '.', 'Dr.', 'Dakshinamurthy', 'V', 'Kolluru', ',', 'Dr.', 'Sridhar', 'Pappu', 'and', 'A', 'S', 'L', 'Ganapathi', 'Kumar', 'started', 'the', 'institution', 'in', 'Hyderabad', 'in', 'mid-2011', 'and', 'expanded', 'to', 'Bengaluru', 'in', 'early-2016', '.', 'Initially', 'the', 'school', 'functioned', 'under', 'mentorship', 'of', 'Dr.', 'Dakshinamurthy', ',', 'Dr.', 'Sridhar', 'and', 'Dr.', 'Sreerama', 'Murthy', '.', 'They', 'are', 'now', 'supported', 'by', 'a', 'team', 'of', 'additional', 'mentors', 'and', 'in-house', 'data', 'scientists', '.', 'In', '2012', ',', 'INSOFE', 'also', 'started', 'Corporate', 'training', 'services', '.', 'It', 'extended', 'operations', 'to', 'Bengaluru', 'in', '2016', '.', 'CIO.com', 'listed', 'INSOFE', '3rd', 'in', 'their', 'list', 'of', '``', '16', 'Big', 'Data', 'Certifications', 'That', 'Will', 'Pay', 'Off', \"''\", 'consecutively', 'from', '2013-2016', '.', 'Silicon', 'India', 'Magazine', 'listed', 'INSOFE', 'in', 'their', 'list', 'of', '``', 'Top', '5', 'Big', 'Data', 'Training', 'Institutes', '2016', \"''\", '.', 'Analytics', 'India', 'Magazine', ',', 'listed', 'INSOFE', 'in', '``', 'Top', '9', 'Analytics', 'Training', 'Institutes', 'in', 'India', 'in', '2016', \"''\", '.', 'KDnuggets', 'mentioned', 'INSOFE', 'in', 'their', 'list', 'of', 'Certificates', 'in', 'Analytics', ',', 'Data', 'Mining', ',', 'and', 'Data', 'Science', 'in', '2014', '.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the data\n",
        "tokens1 = word_tokenize(text)\n",
        "print(tokens1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7lKa49S1KAI"
      },
      "source": [
        "Match pattern starting with I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAelTY6GuW-p",
        "outputId": "b0bc140d-28c5-4d99-acf8-19cade0e66b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['International',\n",
              " 'INSOFE',\n",
              " 'It',\n",
              " 'It',\n",
              " 'Initially',\n",
              " 'In',\n",
              " 'INSOFE',\n",
              " 'It',\n",
              " 'INSOFE',\n",
              " 'India',\n",
              " 'INSOFE',\n",
              " 'Institutes',\n",
              " 'India',\n",
              " 'INSOFE',\n",
              " 'Institutes',\n",
              " 'India',\n",
              " 'INSOFE']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('^I', w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3TD2KE_1KAI"
      },
      "source": [
        "Get all the tokens ending with either ing or uru string\n",
        "ing$|uru$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvfP6sQNuW-s",
        "outputId": "42b13422-a93c-4357-dce0-9af4d053330b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Engineering',\n",
              " 'Engineering',\n",
              " 'Bengaluru',\n",
              " 'working',\n",
              " 'Kolluru',\n",
              " 'Bengaluru',\n",
              " 'training',\n",
              " 'Bengaluru',\n",
              " 'Training',\n",
              " 'Training',\n",
              " 'Mining']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('ing$|uru$', w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hka7F7Ae1KAI"
      },
      "source": [
        "Get all the words that has H,B or M as its first letter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSLIDu_puW-v",
        "outputId": "9ba1a269-bd0c-4d10-d84a-9350e26bb126"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hyderabad',\n",
              " 'Bengaluru',\n",
              " 'Mumbai',\n",
              " 'Hyderabad',\n",
              " 'Bengaluru',\n",
              " 'Murthy',\n",
              " 'Bengaluru',\n",
              " 'Big',\n",
              " 'Magazine',\n",
              " 'Big',\n",
              " 'Magazine',\n",
              " 'Mining']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('^[H|B|M]', w)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_DzIkUy-6e",
        "outputId": "d1faa460-0ab7-4792-a6a8-6d6d478ce0a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hyderabad',\n",
              " 'Bengaluru',\n",
              " 'Mumbai',\n",
              " 'Hyderabad',\n",
              " 'Bengaluru',\n",
              " 'Murthy',\n",
              " 'Bengaluru',\n",
              " 'Big',\n",
              " 'Magazine',\n",
              " 'Big',\n",
              " 'Magazine',\n",
              " 'Mining']"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('^[H|B|M]', w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVvtdB5g1KAJ"
      },
      "source": [
        "Search for words - Hyderabad, Bengaluru and Mumbai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX3UNTHiuW-x",
        "outputId": "d3f7b4ea-ba07-4d99-9ee5-56cb0e79f843"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hyderabad', 'Bengaluru', 'Mumbai', 'Hyderabad', 'Bengaluru', 'Bengaluru']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('^Hyd|Ben|Mum', w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wflwqZexivZS"
      },
      "source": [
        "Search for 'Data', 'Analytics or Science' words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjU6Xk-KuW-z",
        "outputId": "59e53882-fe0c-4d97-92e4-40e24d4bd2fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'Science',\n",
              " 'Data',\n",
              " 'Data',\n",
              " 'Analytics',\n",
              " 'Analytics',\n",
              " 'Analytics',\n",
              " 'Data',\n",
              " 'Data',\n",
              " 'Science']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('Data|Ana|Sci', w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL05t0w_1KAL"
      },
      "source": [
        "Get all the workds that ends with es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vGO3_DruW-1",
        "outputId": "0b122e70-9c72-4fce-b4aa-c679557f7bd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['services', 'Institutes', 'Institutes', 'Certificates']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('es$', w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDC64B_G1KAM"
      },
      "source": [
        "Extract pattern with numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adi4biBduW-3",
        "outputId": "5c6683cc-5f67-40f6-8566-4dee954928f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2011',\n",
              " 'mid-2011',\n",
              " 'early-2016',\n",
              " '2012',\n",
              " '2016',\n",
              " '3rd',\n",
              " '16',\n",
              " '2013-2016',\n",
              " '5',\n",
              " '2016',\n",
              " '9',\n",
              " '2016',\n",
              " '2014']"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w for w in tokens1 if re.search('[0-9]', w)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p5W17v8zsU8",
        "outputId": "2507a4b5-ee79-4cfd-ee7b-8ff95e46e5a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This', 'That']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x='This is this and That is that'\n",
        "tokens2 = word_tokenize(x)\n",
        "[w for w in tokens2 if re.search('^T',w)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFbl2WwI0aqX",
        "outputId": "c098387c-c883-4267-b343-9a21dbe94124"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['THIS', 'IS', 'THIS', 'AND', 'THAT', 'IS', 'THAT']"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[w.upper() for w in tokens2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro_bXcUHuW-_"
      },
      "source": [
        "# Lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTxgzOEiuW_A",
        "outputId": "2f8b46b9-a45f-4a22-e38f-af7a0b2a8af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'the', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'to', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'my', 'companion', 'mr', 'alfred', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows', 'the', 'train', 'was', '09', '30', 'am', 'and', 'we', 'had', 'to', 'reach', 'the', 'station', 'by', '08', '30', 'am', 'at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens']\n"
          ]
        }
      ],
      "source": [
        "tokens = [token.lower() for token in tokens] # Converting list of tokens to lower case\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmS7fLr_uW_C"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFWuY1psuW_C"
      },
      "source": [
        "A stop word is a commonly used word (such as \"a\", \"an“, \"it”, “in”, “the”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
        "\n",
        "We would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words. NLTK (Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory. To check the list of stopwords you can type the following commands in the python shell.\n",
        "\n",
        "Note: You can even modify the list by adding words of your choice in the english .txt. file in the stopwords directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydSa6By2uW_D"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIDi8YRIsdd6",
        "outputId": "1c3e254b-0b72-48b3-bc8e-4f8169473d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acx6qbWm03sy",
        "outputId": "a30af411-2cff-418d-fc4d-367d1361ba51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hk5qeHw1KAP"
      },
      "source": [
        "Stopword removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9L_Xp1E1Jj5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJVVLAIJuW_F",
        "outputId": "62109cd7-37e9-4406-a62d-a179ce9b2af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'Waterloo', 'fortunate', 'catching', 'train', 'Leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'Surrey', 'lanes', 'It', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', 'The', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'To', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'My', 'companion', 'Mr', 'Alfred', 'sat', 'front', 'trap', 'arms', 'folded', 'hat', 'pulled', 'eyes', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'Suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadows', 'The', 'train', '09', '30', 'AM', 'reach', 'station', '08', '30', 'AM', 'At', 'Waterloo', 'fortunate', 'catching', 'train', 'Leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'Surrey', 'lanes', 'It', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens']\n"
          ]
        }
      ],
      "source": [
        "stop = stopwords.words('english')\n",
        "\n",
        "tokens = [token for token in tokens if token not in stop]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_SaHUFK-nPPX",
        "outputId": "1c9754ec-09f5-424d-de01-e8c987133086"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lying'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"lie\"\n",
        "\"lying\"\n",
        "stemming : \"ly\"\n",
        "lemmatization : \"lie\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYZ6J-ZuW_I"
      },
      "source": [
        "# Stemmers and Lemmatizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vElKMcELuW_I"
      },
      "source": [
        "WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing.\n",
        "\n",
        "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinctions. First, WordNet interlinks not just word forms—strings of letters—but specific senses of words. As a result, words that are found in close proximity to one another in the network are semantically disambiguated. Second, WordNet labels the semantic relations among words, whereas the groupings of words in a thesaurus does not follow any explicit pattern other than meaning similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rnyd14tuW_L"
      },
      "source": [
        "#### Stemmers vs. Lemmatizers\n",
        "\n",
        "* Both stemmers and lemmatizers try to bring inflected words to the same form.\n",
        "* Stemmers use an algorithmic approach of removing prefixes and suffixes. The result might not be an actual dictionary word.\n",
        "* Lemmatizers use a corpus. The result is always a dictionary word.\n",
        "* Lemmatizers need extra info about the part of speech they are processing.\n",
        "* Stemmers are faster than lemmatizers\n",
        "\n",
        "When to use stemmers and when to use lemmatizers? few guidelines:\n",
        "* If speed is important, use stemmers (lemmatizers have to search through a corpus while stemmers do simple operations on a string)\n",
        "* If you just want to make sure that the system you are building is tolerant to inflections, use stemmers (If you query for “best bar in New York”, you’d accept an article on “Best bars in New York 2016″)\n",
        "* If you need the actual dictionary word, use a lemmatizer. (for example, if you are building a natural language generation system)\n",
        "\n",
        "How do stemmers work?\n",
        "\n",
        "* Stemmers are extremely simple to use and very fast. They usually are the preferred choice. They work by applying different transformation rules on the word until no other transformation can be applied.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTC3avdNuW_M"
      },
      "source": [
        "### Stemmers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFEJUF9AuW_N"
      },
      "source": [
        "**There are two Stemmer algorithms that can be used for stemming - Porter and Snowball**\n",
        "\n",
        "* Porter: It is the most commonly used stemmer. It is one of the few stemmers that actually have Java support and it is also the most computationally intensive and the oldest algorithm by a large margin.\n",
        "\n",
        "* Snowball: This is an improvement over porter. It is slightly faster in computation time than porter, with a reasonably large community around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dm0267XuW_N",
        "outputId": "7b6c923b-a5bb-4184-f1e6-d14540e5e37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "snow = SnowballStemmer('english')\n",
        "\n",
        "print(snow.languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO6bgPJluW_Q",
        "outputId": "73daa6c9-54a5-4231-94d3-62d2eb2d6763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get\n",
            "rabbit\n",
            "xyze\n",
            "agre\n",
            "slowli\n"
          ]
        }
      ],
      "source": [
        "print(snow.stem('getting'))\n",
        "print(snow.stem('rabbits'))\n",
        "print(snow.stem('xyzing'))\n",
        "print(snow.stem('agreed'))\n",
        "print(snow.stem('slowly'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er-dKAVYuW_S",
        "outputId": "99032308-67b7-4eb1-9594-5c781caac59f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get\n",
            "rabbit\n",
            "xyze\n",
            "agre\n",
            "slowli\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()\n",
        "\n",
        "print(porter.stem('getting'))\n",
        "print(porter.stem('rabbits'))\n",
        "print(porter.stem('xyzing'))\n",
        "print(porter.stem('agreed'))\n",
        "print(porter.stem('slowly'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7RG8p_EuW_V"
      },
      "source": [
        "### Lemmatizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M96-8DFuW_W"
      },
      "source": [
        "One major difference between stemming and lemmatization is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K_Y5CQ9uW_I"
      },
      "source": [
        "Obtain the resources using nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2yc65RY1KAW",
        "outputId": "f6aaae69-97d6-4b8b-bfac-b1004fdf9b43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYCiF1QrzJH_",
        "outputId": "d16b33cf-6817-4f8a-9a2b-9ea524b5e9df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cspxr2kuuW_X",
        "outputId": "d8a9026e-18c4-4d5e-df9d-6ba4b11284cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['going', 'gone', 'go', 'goes', 'went']\n",
            "['go', 'go', 'go', 'go', 'go']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lmtzr = WordNetLemmatizer()\n",
        "\n",
        "tokens_new = [\"going\", \"gone\", \"go\", \"goes\", \"went\"]\n",
        "print(tokens_new)\n",
        "\n",
        "tokens_new = [lmtzr.lemmatize(token, pos='v') for token in tokens_new]\n",
        "print(tokens_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehnj6_Ss1KAW"
      },
      "source": [
        "Stemmer and lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daLvcNXouW_Z"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "lemtzr = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgLQJAk4CnnK"
      },
      "outputs": [],
      "source": [
        "plurals = ['Indian', 'caresses', 'flies', 'dies', 'education', 'denied', 'computer', 'computing', 'xyzing', 'done', 'slept']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzmGo3ZrCjSZ",
        "outputId": "7fab0aa6-8507-46d0-c99a-afd2168677e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['indian', 'caress', 'fli', 'die', 'educ', 'deni', 'comput', 'comput', 'xyze', 'done', 'slept']\n",
            "['Indian', 'caress', 'fly', 'die', 'education', 'deny', 'computer', 'compute', 'xyzing', 'do', 'sleep']\n"
          ]
        }
      ],
      "source": [
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(singles)\n",
        "\n",
        "tokensLmtz = [lemtzr.lemmatize(token, pos='v') for token in plurals]\n",
        "print(tokensLmtz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwPSu4jRuW_c",
        "outputId": "6e067215-6901-4ba1-cb8d-09c0b377bf7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['at', 'waterloo', 'fortun', 'catch', 'train', 'leatherhead', 'hire', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'love', 'surrey', 'lane', 'it', 'perfect', 'day', 'bright', 'sun', 'fleeci', 'cloud', 'heaven', 'the', 'tree', 'waysid', 'hedg', 'throw', 'first', 'green', 'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'to', 'least', 'strang', 'contrast', 'sweet', 'promis', 'spring', 'sinist', 'quest', 'upon', 'engag', 'my', 'companion', 'mr', 'alfr', 'sat', 'front', 'trap', 'arm', 'fold', 'hat', 'pull', 'eye', 'chin', 'sunk', 'upon', 'breast', 'buri', 'deepest', 'thought', 'suddenli', 'howev', 'start', 'tap', 'shoulder', 'point', 'meadow', 'the', 'train', '09', '30', 'am', 'reach', 'station', '08', '30', 'am', 'at', 'waterloo', 'fortun', 'catch', 'train', 'leatherhead', 'hire', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'love', 'surrey', 'lane', 'it', 'perfect', 'day', 'bright', 'sun', 'fleeci', 'cloud', 'heaven']\n",
            "['At', 'Waterloo', 'fortunate', 'catch', 'train', 'Leatherhead', 'hire', 'trap', 'station', 'inn', 'drive', 'four', 'five', 'miles', 'lovely', 'Surrey', 'lanes', 'It', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heavens', 'The', 'tree', 'wayside', 'hedge', 'throw', 'first', 'green', 'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'To', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engage', 'My', 'companion', 'Mr', 'Alfred', 'sit', 'front', 'trap', 'arm', 'fold', 'hat', 'pull', 'eye', 'chin', 'sink', 'upon', 'breast', 'bury', 'deepest', 'think', 'Suddenly', 'however', 'start', 'tap', 'shoulder', 'point', 'meadows', 'The', 'train', '09', '30', 'AM', 'reach', 'station', '08', '30', 'AM', 'At', 'Waterloo', 'fortunate', 'catch', 'train', 'Leatherhead', 'hire', 'trap', 'station', 'inn', 'drive', 'four', 'five', 'miles', 'lovely', 'Surrey', 'lanes', 'It', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heavens']\n"
          ]
        }
      ],
      "source": [
        "porter_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(porter_tokens)\n",
        "\n",
        "lmtzr_tokens = [lmtzr.lemmatize(token, pos='v') for token in tokens]\n",
        "print(lmtzr_tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RnlF5NJ1uW-M"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}